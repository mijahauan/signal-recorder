#!/usr/bin/env python3
"""
Phase 2 Analytics Service - Continuous Processing of Digital RF Archives

================================================================================
PURPOSE
================================================================================
The Phase 2 Analytics Service is the RUNTIME WRAPPER that continuously monitors
the Phase 1 Digital RF archive and processes new data through the Phase 2
Temporal Analysis Engine.

This service runs as a systemd daemon (grape-phase2-analytics.service) and:
    1. Polls for new minute-aligned data in the raw archive
    2. Invokes Phase2TemporalEngine.process_minute() for each new minute
    3. Writes results to CSV time series files
    4. Updates status JSON for web-ui monitoring
    5. Manages decimation buffer for Phase 3 upload

================================================================================
ARCHITECTURE: SERVICE vs ENGINE
================================================================================
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Phase2AnalyticsService (THIS FILE)                      â”‚
â”‚                                                                             â”‚
â”‚   RESPONSIBILITIES:                                                         â”‚
â”‚   - Daemon lifecycle (start, stop, signal handling)                         â”‚
â”‚   - Archive polling and data retrieval                                      â”‚
â”‚   - CSV time series management (per-method files)                           â”‚
â”‚   - Status file updates for web-ui                                          â”‚
â”‚   - Clock convergence model integration                                     â”‚
â”‚   - Decimation buffer for Phase 3                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Phase2TemporalEngine (phase2_temporal_engine.py)            â”‚
â”‚                                                                             â”‚
â”‚   RESPONSIBILITIES:                                                         â”‚
â”‚   - Tone detection (Step 1)                                                 â”‚
â”‚   - Channel characterization (Step 2)                                       â”‚
â”‚   - Transmission time solution (Step 3)                                     â”‚
â”‚   - D_clock computation                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

================================================================================
DATA FLOW
================================================================================
                        Phase 1 Archive
                              â”‚
    raw_archive/{CHANNEL}/    â”‚   (Digital RF HDF5 files)
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Phase2AnalyticsService.run()                            â”‚
â”‚                                                                             â”‚
â”‚   1. Poll for new minute-aligned data                                       â”‚
â”‚   2. Read IQ samples from Digital RF                                        â”‚
â”‚   3. Call engine.process_minute(iq_samples, system_time, rtp_timestamp)     â”‚
â”‚   4. Write results to CSV files                                             â”‚
â”‚   5. Update status JSON                                                     â”‚
â”‚   6. Write decimated 10 Hz data to buffer                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                     Phase 2 Output Directory
                              â”‚
    phase2/{CHANNEL}/         â”‚
    â”œâ”€â”€ clock_offset/         â”‚   clock_offset_series.csv
    â”œâ”€â”€ carrier_power/        â”‚   carrier_power_{date}.csv
    â”œâ”€â”€ tone_detections/      â”‚   {channel}_tones_{date}.csv
    â”œâ”€â”€ bcd_discrimination/   â”‚   {channel}_bcd_{date}.csv
    â”œâ”€â”€ doppler/              â”‚   {channel}_doppler_{date}.csv
    â”œâ”€â”€ station_id_440hz/     â”‚   {channel}_440hz_{date}.csv
    â”œâ”€â”€ test_signal/          â”‚   {channel}_test_{date}.csv
    â”œâ”€â”€ discrimination/       â”‚   {channel}_discrimination_{date}.csv
    â”œâ”€â”€ audio_tones/          â”‚   {channel}_audio_{date}.csv
    â””â”€â”€ status/               â”‚   analytics-service-status.json

================================================================================
CSV TIME SERIES FILES
================================================================================
Each discrimination method produces its own CSV for visualization:

FILE                          | DESCRIPTION
------------------------------|---------------------------------------------
clock_offset_series.csv       | D_clock, propagation mode, quality grade
carrier_power_{date}.csv      | Power/SNR measurements
{channel}_tones_{date}.csv    | 1000/1200 Hz detection results
{channel}_bcd_{date}.csv      | BCD correlation amplitudes and delays
{channel}_doppler_{date}.csv  | Doppler shift and stability
{channel}_440hz_{date}.csv    | 440 Hz tone and ground truth detection
{channel}_test_{date}.csv     | Test signal analysis (minutes 8/44)
{channel}_discrimination.csv  | Final weighted voting result

================================================================================
CLOCK CONVERGENCE MODEL
================================================================================
The service integrates a ClockConvergenceModel that implements:

    "SET, MONITOR, INTERVENTION"

1. SET (Acquisition): Collect D_clock measurements, compute running mean/std
2. MONITOR (Lock): When uncertainty < 1ms, lock and flag anomalies
3. INTERVENTION (Reacquire): Force reacquisition after consecutive anomalies

This provides:
    - Stable D_clock output (smooth over short-term ionospheric variations)
    - Anomaly detection (flag propagation mode changes, ionospheric events)
    - Quality grading based on convergence state

================================================================================
USAGE
================================================================================
The service is typically started via systemd:

    systemctl start grape-phase2-analytics@WWV_10_MHz

Or directly for testing:

    python -m grape_recorder.grape.phase2_analytics_service \\
        --archive /data/raw_archive/WWV_10_MHz \\
        --output /data/phase2/WWV_10_MHz \\
        --channel "WWV 10 MHz" \\
        --frequency 10e6 \\
        --grid EM38ww

================================================================================
REVISION HISTORY
================================================================================
2025-12-07: Added comprehensive service architecture documentation
2025-12-01: Added clock convergence model integration
2025-11-20: Added per-method CSV files for web-ui graphs
2025-10-15: Initial implementation with Phase2TemporalEngine integration
"""

import argparse
import csv
import json
import logging
import signal
import sys
import time
import numpy as np
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple

logger = logging.getLogger(__name__)


class Phase2AnalyticsService:
    """
    Phase 2 Analytics Service - reads DRF, produces timing analysis.
    
    Monitors raw_archive/{CHANNEL}/ for new Digital RF data and
    processes each minute through Phase2TemporalEngine.
    """
    
    def __init__(
        self,
        archive_dir: Path,
        output_dir: Path,
        channel_name: str,
        frequency_hz: float,
        sample_rate: int = 20000,
        receiver_grid: str = '',
        station_config: Optional[Dict] = None,
        poll_interval: float = 10.0
    ):
        """
        Initialize Phase 2 analytics service.
        
        Args:
            archive_dir: Directory containing Digital RF from Phase 1
            output_dir: Output directory for Phase 2 products
            channel_name: Channel identifier
            frequency_hz: Center frequency in Hz
            sample_rate: Sample rate (default 20000)
            receiver_grid: Receiver grid square for propagation calculations
            station_config: Station metadata
            poll_interval: Seconds between polling for new data
        """
        self.archive_dir = Path(archive_dir)
        self.output_dir = Path(output_dir)
        self.channel_name = channel_name
        self.frequency_hz = frequency_hz
        self.sample_rate = sample_rate
        self.receiver_grid = receiver_grid
        self.station_config = station_config or {}
        self.poll_interval = poll_interval
        
        # Create output directories using coordinated path structure
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.status_dir = self.output_dir / 'status'
        self.status_dir.mkdir(parents=True, exist_ok=True)
        
        # Clock offset series directory: {data_root}/phase2/{CHANNEL}/clock_offset/
        self.clock_offset_dir = self.output_dir / 'clock_offset'
        self.clock_offset_dir.mkdir(parents=True, exist_ok=True)
        
        # Status file for web-ui
        self.status_file = self.status_dir / 'analytics-service-status.json'
        
        # CSV time series for D_clock (coordinated path)
        self.clock_offset_csv = self.clock_offset_dir / 'clock_offset_series.csv'
        self._init_clock_offset_csv()
        
        # CSV time series for carrier power (for power graphs)
        self.carrier_power_dir = self.output_dir / 'carrier_power'
        self.carrier_power_dir.mkdir(parents=True, exist_ok=True)
        self._init_carrier_power_csv()
        
        # ====================================================================
        # Discrimination Method CSV Directories
        # ====================================================================
        
        # Tone detections (1000/1200 Hz timing tones)
        self.tone_detections_dir = self.output_dir / 'tone_detections'
        self.tone_detections_dir.mkdir(parents=True, exist_ok=True)
        self._init_tone_detections_csv()
        
        # BCD discrimination (BCD correlation analysis)
        self.bcd_discrimination_dir = self.output_dir / 'bcd_discrimination'
        self.bcd_discrimination_dir.mkdir(parents=True, exist_ok=True)
        self._init_bcd_discrimination_csv()
        
        # Doppler analysis
        self.doppler_dir = self.output_dir / 'doppler'
        self.doppler_dir.mkdir(parents=True, exist_ok=True)
        self._init_doppler_csv()
        
        # Station ID (440 Hz voice ID + 500/600 Hz ground truth)
        self.station_id_dir = self.output_dir / 'station_id_440hz'
        self.station_id_dir.mkdir(parents=True, exist_ok=True)
        self._init_station_id_csv()
        
        # Test signal (minutes 8 and 44)
        self.test_signal_dir = self.output_dir / 'test_signal'
        self.test_signal_dir.mkdir(parents=True, exist_ok=True)
        self._init_test_signal_csv()
        
        # Discrimination summary (weighted voting result)
        self.discrimination_dir = self.output_dir / 'discrimination'
        self.discrimination_dir.mkdir(parents=True, exist_ok=True)
        self._init_discrimination_csv()
        
        # Audio tone monitor (500/600 Hz + intermodulation)
        self.audio_tones_dir = self.output_dir / 'audio_tones'
        self.audio_tones_dir.mkdir(parents=True, exist_ok=True)
        self._init_audio_tones_csv()
        
        # Decimated 10 Hz output buffer (Phase 3 products)
        from .decimated_buffer import DecimatedBuffer
        self.decimated_buffer = DecimatedBuffer(self.output_dir.parent.parent, channel_name)
        
        # Initialize stateful decimation filter (preserves state across minute boundaries)
        from .decimation import StatefulDecimator, is_rate_supported
        if is_rate_supported(sample_rate):
            self.decimator = StatefulDecimator(sample_rate, 10)
            logger.info(f"  Decimation: {sample_rate} Hz â†’ 10 Hz enabled (stateful)")
        else:
            self.decimator = None
            logger.warning(f"  Decimation: {sample_rate} Hz not supported")
        
        # Decimation parameters
        self.decimation_factor = int(sample_rate / 10)  # 2000 for 20kHz
        self.output_rate = 10  # 10 Hz output
        
        # Initialize Phase 2 engine
        from .phase2_temporal_engine import Phase2TemporalEngine
        
        # Extract precise coordinates from station_config if available
        # Precise coordinates improve timing accuracy by ~16Î¼s over grid square center
        precise_lat = self.station_config.get('latitude')
        precise_lon = self.station_config.get('longitude')
        
        if precise_lat is not None and precise_lon is not None:
            logger.info(f"Using precise coordinates: {precise_lat:.6f}Â°N, {precise_lon:.6f}Â°W")
        
        self.engine = Phase2TemporalEngine(
            raw_archive_dir=self.archive_dir.parent,  # parent contains all channels
            output_dir=self.output_dir,
            channel_name=channel_name,
            frequency_hz=frequency_hz,
            receiver_grid=receiver_grid,
            sample_rate=sample_rate,
            precise_lat=precise_lat,
            precise_lon=precise_lon
        )
        
        # Initialize Clock Convergence Model
        # "Set, Monitor, Intervention" architecture for GPSDO-disciplined timing
        from .clock_convergence import ClockConvergenceModel
        
        convergence_state_file = self.status_dir / 'convergence_state.json'
        self.convergence_model = ClockConvergenceModel(
            lock_uncertainty_ms=1.0,      # Lock when uncertainty < 1ms
            min_samples_for_lock=30,      # Need 30 minutes of data
            anomaly_sigma=3.0,            # 3Ïƒ for anomaly detection
            max_consecutive_anomalies=5,  # Force reacquire after 5 anomalies
            state_file=convergence_state_file
        )
        logger.info(f"Initialized clock convergence model (state file: {convergence_state_file})")
        
        # State tracking
        self.running = False
        self.start_time = time.time()
        self.minutes_processed = 0
        self.last_processed_minute = 0
        self.last_result = None
        self.last_carrier_snr_db = None  # Carrier SNR from IQ data
        self.last_carrier_power_db = None  # Carrier power from IQ data
        
        # Track which minutes we've processed
        self.processed_minutes = set()
        
        logger.info(f"Phase2AnalyticsService initialized for {channel_name}")
        logger.info(f"  Archive: {archive_dir}")
        logger.info(f"  Output: {output_dir}")
        logger.info(f"  Frequency: {frequency_hz/1e6:.3f} MHz")
        logger.info(f"  Grid: {receiver_grid}")
    
    def _init_clock_offset_csv(self):
        """Initialize clock offset CSV file with headers if needed."""
        if not self.clock_offset_csv.exists():
            with open(self.clock_offset_csv, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'system_time', 'utc_time', 'minute_boundary_utc',
                    'clock_offset_ms', 'station', 'frequency_mhz',
                    'propagation_delay_ms', 'propagation_mode', 'n_hops',
                    'confidence', 'uncertainty_ms', 'quality_grade',
                    'snr_db', 'delay_spread_ms', 'doppler_std_hz', 'fss_db',
                    'wwv_power_db', 'wwvh_power_db', 'discrimination_confidence',
                    'utc_verified', 'multi_station_verified',
                    'rtp_timestamp', 'processed_at'
                ])
            logger.info(f"Created clock offset CSV: {self.clock_offset_csv}")
    
    def _write_clock_offset(self, result, minute_boundary: int, rtp_timestamp: int):
        """Append D_clock measurement to CSV time series with convergence tracking."""
        try:
            # Extract values from Phase2Result
            solution = result.solution if hasattr(result, 'solution') else None
            station = solution.station if solution else 'UNKNOWN'
            frequency_mhz = self.frequency_hz / 1e6
            
            # ================================================================
            # Process through Clock Convergence Model
            # "Set, Monitor, Intervention" - converge to lock, then monitor
            # ================================================================
            # Derive quality_grade from uncertainty_ms for convergence model
            unc = result.uncertainty_ms
            input_grade = 'A' if unc < 1.0 else 'B' if unc < 3.0 else 'C' if unc < 10.0 else 'D'
            
            convergence_result = self.convergence_model.process_measurement(
                station=station,
                frequency_mhz=frequency_mhz,
                d_clock_ms=result.d_clock_ms,
                timestamp=float(minute_boundary),
                snr_db=self.last_carrier_snr_db,
                quality_grade=input_grade
            )
            
            # Use converged values when locked, raw values otherwise
            if convergence_result.is_locked:
                effective_d_clock = convergence_result.d_clock_ms
                effective_uncertainty = convergence_result.uncertainty_ms
                quality_grade = 'A' if convergence_result.uncertainty_ms < 0.5 else 'B'
            else:
                effective_d_clock = convergence_result.d_clock_ms  # Running mean
                effective_uncertainty = convergence_result.uncertainty_ms
                # Grade based on convergence progress
                progress = convergence_result.convergence_progress
                if progress >= 0.9:
                    quality_grade = 'B'
                elif progress >= 0.5:
                    quality_grade = 'C'
                else:
                    quality_grade = 'D'
            
            # Log convergence state changes
            if convergence_result.is_locked and convergence_result.sample_count == 30:
                logger.info(
                    f"ğŸ”’ LOCKED: {self.channel_name} D_clock = "
                    f"{effective_d_clock:.3f} Â± {effective_uncertainty:.3f} ms"
                )
            
            # Log anomalies (propagation events!)
            if convergence_result.is_anomaly:
                logger.info(
                    f"ğŸ“¡ PROPAGATION EVENT: {self.channel_name} residual = "
                    f"{convergence_result.residual_ms:.2f} ms "
                    f"({convergence_result.anomaly_sigma:.1f}Ïƒ)"
                )
            
            with open(self.clock_offset_csv, 'a', newline='') as f:
                writer = csv.writer(f)
                
                writer.writerow([
                    minute_boundary,                                    # system_time
                    minute_boundary + (effective_d_clock / 1000.0),     # utc_time
                    minute_boundary,                                    # minute_boundary_utc
                    effective_d_clock,                                  # clock_offset_ms (converged)
                    station,                                            # station
                    frequency_mhz,                                      # frequency_mhz
                    solution.t_propagation_ms if solution else 0,       # propagation_delay_ms
                    solution.propagation_mode if solution else '',      # propagation_mode
                    solution.n_hops if solution else 0,                 # n_hops
                    solution.confidence if solution else 0,             # confidence
                    effective_uncertainty,                              # uncertainty_ms (from convergence)
                    quality_grade,                                      # quality_grade (from convergence)
                    self.last_carrier_snr_db or 0,                      # snr_db
                    '',                                                 # delay_spread_ms
                    '',                                                 # doppler_std_hz
                    '',                                                 # fss_db
                    '',                                                 # wwv_power_db
                    '',                                                 # wwvh_power_db
                    '',                                                 # discrimination_confidence
                    convergence_result.is_locked,                       # utc_verified (locked = verified)
                    False,                                              # multi_station_verified
                    rtp_timestamp,                                      # rtp_timestamp
                    datetime.now(timezone.utc).timestamp()              # processed_at
                ])
                
            # Store convergence result for status reporting
            self.last_convergence_result = convergence_result
            
        except Exception as e:
            logger.error(f"Failed to write clock offset: {e}")
    
    def _init_carrier_power_csv(self):
        """Initialize carrier power CSV file for today."""
        today = datetime.now(timezone.utc).strftime('%Y%m%d')
        self.carrier_power_csv = self.carrier_power_dir / f'carrier_power_{today}.csv'
        
        if not self.carrier_power_csv.exists():
            with open(self.carrier_power_csv, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'timestamp', 'utc_time', 'power_db', 'snr_db',
                    'wwv_tone_db', 'wwvh_tone_db', 'station', 'quality_grade'
                ])
            logger.info(f"Created carrier power CSV: {self.carrier_power_csv}")
    
    def _write_carrier_power(self, minute_boundary: int, power_db: float, snr_db: float,
                              wwv_tone_db: float = None, wwvh_tone_db: float = None,
                              station: str = None, quality_grade: str = None):
        """Append carrier power measurement to daily CSV."""
        try:
            # Ensure we're writing to today's file
            today = datetime.now(timezone.utc).strftime('%Y%m%d')
            expected_csv = self.carrier_power_dir / f'carrier_power_{today}.csv'
            if self.carrier_power_csv != expected_csv:
                self.carrier_power_csv = expected_csv
                self._init_carrier_power_csv()
            
            with open(self.carrier_power_csv, 'a', newline='') as f:
                writer = csv.writer(f)
                utc_time = datetime.fromtimestamp(minute_boundary, timezone.utc).isoformat()
                writer.writerow([
                    minute_boundary,
                    utc_time,
                    round(power_db, 2) if power_db else '',
                    round(snr_db, 2) if snr_db else '',
                    round(wwv_tone_db, 2) if wwv_tone_db else '',
                    round(wwvh_tone_db, 2) if wwvh_tone_db else '',
                    station or '',
                    quality_grade or ''
                ])
        except Exception as e:
            logger.error(f"Failed to write carrier power: {e}")
    
    # ========================================================================
    # Discrimination Method CSV Writers
    # ========================================================================
    
    def _get_file_channel_name(self) -> str:
        """Get filename-safe channel name (spaces and dots to underscores)."""
        return self.channel_name.replace(' ', '_').replace('.', '_')
    
    def _init_tone_detections_csv(self):
        """Initialize tone detections CSV for today."""
        today = datetime.now(timezone.utc).strftime('%Y%m%d')
        file_channel = self._get_file_channel_name()
        self.tone_detections_csv = self.tone_detections_dir / f'{file_channel}_tones_{today}.csv'
        
        if not self.tone_detections_csv.exists():
            with open(self.tone_detections_csv, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'timestamp_utc', 'minute_boundary', 'wwv_detected', 'wwvh_detected',
                    'wwv_snr_db', 'wwvh_snr_db', 'wwv_timing_ms', 'wwvh_timing_ms',
                    'anchor_station', 'anchor_confidence'
                ])
            logger.info(f"Created tone detections CSV: {self.tone_detections_csv}")
    
    def _write_tone_detections(self, minute_boundary: int, time_snap):
        """Write tone detection results from TimeSnapResult."""
        try:
            today = datetime.now(timezone.utc).strftime('%Y%m%d')
            file_channel = self._get_file_channel_name()
            expected_csv = self.tone_detections_dir / f'{file_channel}_tones_{today}.csv'
            if self.tone_detections_csv != expected_csv:
                self.tone_detections_csv = expected_csv
                self._init_tone_detections_csv()
            
            with open(self.tone_detections_csv, 'a', newline='') as f:
                writer = csv.writer(f)
                utc_time = datetime.fromtimestamp(minute_boundary, timezone.utc).isoformat()
                writer.writerow([
                    utc_time,
                    minute_boundary,
                    1 if time_snap.wwv_detected else 0,
                    1 if time_snap.wwvh_detected else 0,
                    round(time_snap.wwv_snr_db, 2) if time_snap.wwv_snr_db else '',
                    round(time_snap.wwvh_snr_db, 2) if time_snap.wwvh_snr_db else '',
                    round(time_snap.wwv_timing_ms, 3) if time_snap.wwv_timing_ms else '',
                    round(time_snap.wwvh_timing_ms, 3) if time_snap.wwvh_timing_ms else '',
                    time_snap.anchor_station or '',
                    round(time_snap.anchor_confidence, 3) if time_snap.anchor_confidence else ''
                ])
        except Exception as e:
            logger.error(f"Failed to write tone detections: {e}")
    
    def _init_bcd_discrimination_csv(self):
        """Initialize BCD discrimination CSV for today."""
        today = datetime.now(timezone.utc).strftime('%Y%m%d')
        file_channel = self._get_file_channel_name()
        self.bcd_discrimination_csv = self.bcd_discrimination_dir / f'{file_channel}_bcd_{today}.csv'
        
        if not self.bcd_discrimination_csv.exists():
            with open(self.bcd_discrimination_csv, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'timestamp_utc', 'minute_boundary', 'wwv_amplitude', 'wwvh_amplitude',
                    'differential_delay_ms', 'correlation_quality', 'wwv_toa_ms', 'wwvh_toa_ms',
                    'amplitude_ratio_db'
                ])
            logger.info(f"Created BCD discrimination CSV: {self.bcd_discrimination_csv}")
    
    def _write_bcd_discrimination(self, minute_boundary: int, channel_char):
        """Write BCD discrimination results from ChannelCharacterization."""
        try:
            today = datetime.now(timezone.utc).strftime('%Y%m%d')
            file_channel = self._get_file_channel_name()
            expected_csv = self.bcd_discrimination_dir / f'{file_channel}_bcd_{today}.csv'
            if self.bcd_discrimination_csv != expected_csv:
                self.bcd_discrimination_csv = expected_csv
                self._init_bcd_discrimination_csv()
            
            # Calculate amplitude ratio in dB
            ratio_db = None
            if channel_char.bcd_wwv_amplitude and channel_char.bcd_wwvh_amplitude:
                if channel_char.bcd_wwvh_amplitude > 0:
                    ratio_db = 20 * np.log10(channel_char.bcd_wwv_amplitude / channel_char.bcd_wwvh_amplitude)
            
            with open(self.bcd_discrimination_csv, 'a', newline='') as f:
                writer = csv.writer(f)
                utc_time = datetime.fromtimestamp(minute_boundary, timezone.utc).isoformat()
                writer.writerow([
                    utc_time,
                    minute_boundary,
                    round(channel_char.bcd_wwv_amplitude, 4) if channel_char.bcd_wwv_amplitude else '',
                    round(channel_char.bcd_wwvh_amplitude, 4) if channel_char.bcd_wwvh_amplitude else '',
                    round(channel_char.bcd_differential_delay_ms, 3) if channel_char.bcd_differential_delay_ms else '',
                    round(channel_char.bcd_correlation_quality, 3) if channel_char.bcd_correlation_quality else '',
                    round(channel_char.bcd_wwv_toa_ms, 3) if channel_char.bcd_wwv_toa_ms else '',
                    round(channel_char.bcd_wwvh_toa_ms, 3) if channel_char.bcd_wwvh_toa_ms else '',
                    round(ratio_db, 2) if ratio_db else ''
                ])
        except Exception as e:
            logger.error(f"Failed to write BCD discrimination: {e}")
    
    def _init_doppler_csv(self):
        """Initialize Doppler analysis CSV for today."""
        today = datetime.now(timezone.utc).strftime('%Y%m%d')
        file_channel = self._get_file_channel_name()
        self.doppler_csv = self.doppler_dir / f'{file_channel}_doppler_{today}.csv'
        
        if not self.doppler_csv.exists():
            with open(self.doppler_csv, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'timestamp_utc', 'minute_boundary', 'wwv_doppler_hz', 'wwvh_doppler_hz',
                    'wwv_doppler_std_hz', 'wwvh_doppler_std_hz', 'doppler_quality',
                    'max_coherent_window_sec', 'phase_variance_rad'
                ])
            logger.info(f"Created Doppler CSV: {self.doppler_csv}")
    
    def _write_doppler(self, minute_boundary: int, channel_char):
        """Write Doppler analysis results from ChannelCharacterization."""
        try:
            today = datetime.now(timezone.utc).strftime('%Y%m%d')
            file_channel = self._get_file_channel_name()
            expected_csv = self.doppler_dir / f'{file_channel}_doppler_{today}.csv'
            if self.doppler_csv != expected_csv:
                self.doppler_csv = expected_csv
                self._init_doppler_csv()
            
            with open(self.doppler_csv, 'a', newline='') as f:
                writer = csv.writer(f)
                utc_time = datetime.fromtimestamp(minute_boundary, timezone.utc).isoformat()
                writer.writerow([
                    utc_time,
                    minute_boundary,
                    round(channel_char.doppler_wwv_hz, 4) if channel_char.doppler_wwv_hz else '',
                    round(channel_char.doppler_wwvh_hz, 4) if channel_char.doppler_wwvh_hz else '',
                    round(channel_char.doppler_wwv_std_hz, 4) if channel_char.doppler_wwv_std_hz else '',
                    round(channel_char.doppler_wwvh_std_hz, 4) if channel_char.doppler_wwvh_std_hz else '',
                    round(channel_char.doppler_quality, 3) if channel_char.doppler_quality else '',
                    round(channel_char.max_coherent_window_sec, 3) if channel_char.max_coherent_window_sec else '',
                    round(channel_char.phase_variance_rad, 6) if channel_char.phase_variance_rad else ''
                ])
        except Exception as e:
            logger.error(f"Failed to write Doppler: {e}")
    
    def _init_station_id_csv(self):
        """Initialize station ID (440Hz/500Hz/600Hz) CSV for today."""
        today = datetime.now(timezone.utc).strftime('%Y%m%d')
        file_channel = self._get_file_channel_name()
        self.station_id_csv = self.station_id_dir / f'{file_channel}_440hz_{today}.csv'
        
        if not self.station_id_csv.exists():
            with open(self.station_id_csv, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'timestamp_utc', 'minute_boundary', 'minute_number',
                    'ground_truth_station', 'ground_truth_source', 'ground_truth_power_db',
                    'station_confidence', 'dominant_station',
                    'harmonic_ratio_500_1000', 'harmonic_ratio_600_1200'
                ])
            logger.info(f"Created station ID CSV: {self.station_id_csv}")
    
    def _write_station_id(self, minute_boundary: int, channel_char):
        """Write station ID results from ChannelCharacterization.
        
        Only writes for minutes 1 (WWVH 440 Hz) and 2 (WWV 440 Hz).
        This CSV is specifically for 440 Hz voice announcement detection.
        """
        try:
            # Calculate minute number within hour (0-59)
            minute_number = (minute_boundary // 60) % 60
            
            # Only write for 440 Hz minutes: 1 = WWVH, 2 = WWV
            if minute_number not in [1, 2]:
                return  # Skip - not a 440 Hz minute
            
            today = datetime.now(timezone.utc).strftime('%Y%m%d')
            file_channel = self._get_file_channel_name()
            expected_csv = self.station_id_dir / f'{file_channel}_440hz_{today}.csv'
            if self.station_id_csv != expected_csv:
                self.station_id_csv = expected_csv
                self._init_station_id_csv()
            
            with open(self.station_id_csv, 'a', newline='') as f:
                writer = csv.writer(f)
                utc_time = datetime.fromtimestamp(minute_boundary, timezone.utc).isoformat()
                writer.writerow([
                    utc_time,
                    minute_boundary,
                    minute_number,
                    channel_char.ground_truth_station or '',
                    channel_char.ground_truth_source or '',
                    round(channel_char.ground_truth_power_db, 2) if channel_char.ground_truth_power_db else '',
                    channel_char.station_confidence or '',
                    channel_char.dominant_station or '',
                    round(channel_char.harmonic_ratio_500_1000, 2) if channel_char.harmonic_ratio_500_1000 else '',
                    round(channel_char.harmonic_ratio_600_1200, 2) if channel_char.harmonic_ratio_600_1200 else ''
                ])
        except Exception as e:
            logger.error(f"Failed to write station ID: {e}")
    
    def _init_test_signal_csv(self):
        """Initialize test signal CSV for today."""
        today = datetime.now(timezone.utc).strftime('%Y%m%d')
        file_channel = self._get_file_channel_name()
        self.test_signal_csv = self.test_signal_dir / f'{file_channel}_test_signal_{today}.csv'
        
        if not self.test_signal_csv.exists():
            with open(self.test_signal_csv, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'timestamp_utc', 'minute_boundary', 'minute_number', 'detected', 'station',
                    'confidence', 'multitone_score', 'chirp_score', 'snr_db',
                    'fss_db', 'delay_spread_ms', 'toa_offset_ms', 'coherence_time_sec'
                ])
            logger.info(f"Created test signal CSV: {self.test_signal_csv}")
    
    def _write_test_signal(self, minute_boundary: int, iq_samples, minute_number: int):
        """Detect and write test signal for minutes 8 and 44."""
        try:
            today = datetime.now(timezone.utc).strftime('%Y%m%d')
            file_channel = self._get_file_channel_name()
            expected_csv = self.test_signal_dir / f'{file_channel}_test_signal_{today}.csv'
            if self.test_signal_csv != expected_csv:
                self.test_signal_csv = expected_csv
                self._init_test_signal_csv()
            
            # Detect test signal using the engine's discriminator
            detection = self.engine.discriminator.test_signal_detector.detect(
                iq_samples=iq_samples,
                minute_number=minute_number,
                sample_rate=self.sample_rate
            )
            
            # Determine station from schedule: minute 8 = WWV, minute 44 = WWVH
            station = 'WWV' if minute_number == 8 else 'WWVH'
            
            with open(self.test_signal_csv, 'a', newline='') as f:
                writer = csv.writer(f)
                utc_time = datetime.fromtimestamp(minute_boundary, timezone.utc).isoformat()
                writer.writerow([
                    utc_time,
                    minute_boundary,
                    minute_number,
                    1 if detection.detected else 0,
                    station if detection.detected else '',
                    round(detection.confidence, 4) if detection.confidence else '',
                    round(detection.multitone_score, 4) if detection.multitone_score else '',
                    round(detection.chirp_score, 4) if detection.chirp_score else '',
                    round(detection.snr_db, 2) if detection.snr_db else '',
                    round(detection.frequency_selectivity_db, 2) if detection.frequency_selectivity_db else '',
                    round(detection.delay_spread_ms, 3) if detection.delay_spread_ms else '',
                    round(detection.toa_offset_ms, 3) if detection.toa_offset_ms else '',
                    round(detection.coherence_time_sec, 3) if detection.coherence_time_sec else ''
                ])
            
            if detection.detected:
                logger.info(
                    f"Test signal detected minute {minute_number}: {station}, "
                    f"confidence={detection.confidence:.2f}, SNR={detection.snr_db:.1f}dB"
                )
        except Exception as e:
            logger.error(f"Failed to write test signal: {e}")
    
    def _init_discrimination_csv(self):
        """Initialize discrimination summary CSV for today."""
        today = datetime.now(timezone.utc).strftime('%Y%m%d')
        file_channel = self._get_file_channel_name()
        self.discrimination_csv = self.discrimination_dir / f'{file_channel}_discrimination_{today}.csv'
        
        if not self.discrimination_csv.exists():
            with open(self.discrimination_csv, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'timestamp_utc', 'minute_boundary', 'dominant_station', 'station_confidence',
                    'wwv_snr_db', 'wwvh_snr_db', 'power_ratio_db', 'ground_truth_station',
                    'quality_grade', 'method_agreements', 'method_disagreements'
                ])
            logger.info(f"Created discrimination CSV: {self.discrimination_csv}")
    
    def _write_discrimination(self, minute_boundary: int, result, time_snap, channel_char):
        """Write discrimination summary combining all methods."""
        try:
            today = datetime.now(timezone.utc).strftime('%Y%m%d')
            file_channel = self._get_file_channel_name()
            expected_csv = self.discrimination_dir / f'{file_channel}_discrimination_{today}.csv'
            if self.discrimination_csv != expected_csv:
                self.discrimination_csv = expected_csv
                self._init_discrimination_csv()
            
            # Calculate power ratio from tone SNRs
            power_ratio_db = None
            if time_snap.wwv_snr_db is not None and time_snap.wwvh_snr_db is not None:
                power_ratio_db = time_snap.wwv_snr_db - time_snap.wwvh_snr_db
            
            # Compute quality_grade from uncertainty_ms
            grade = ''
            if result:
                unc = result.uncertainty_ms
                grade = 'A' if unc < 1.0 else 'B' if unc < 3.0 else 'C' if unc < 10.0 else 'D'
            
            with open(self.discrimination_csv, 'a', newline='') as f:
                writer = csv.writer(f)
                utc_time = datetime.fromtimestamp(minute_boundary, timezone.utc).isoformat()
                writer.writerow([
                    utc_time,
                    minute_boundary,
                    channel_char.dominant_station or '',
                    channel_char.station_confidence or '',
                    round(time_snap.wwv_snr_db, 2) if time_snap.wwv_snr_db else '',
                    round(time_snap.wwvh_snr_db, 2) if time_snap.wwvh_snr_db else '',
                    round(power_ratio_db, 2) if power_ratio_db else '',
                    channel_char.ground_truth_station or '',
                    grade,
                    ';'.join(channel_char.cross_validation_agreements) if channel_char.cross_validation_agreements else '',
                    ';'.join(channel_char.cross_validation_disagreements) if channel_char.cross_validation_disagreements else ''
                ])
        except Exception as e:
            logger.error(f"Failed to write discrimination: {e}")
    
    def _init_audio_tones_csv(self):
        """Initialize audio tones CSV for continuous 500/600 Hz + intermodulation monitoring."""
        today = datetime.now(timezone.utc).strftime('%Y%m%d')
        file_channel = self._get_file_channel_name()
        self.audio_tones_csv = self.audio_tones_dir / f'{file_channel}_audio_tones_{today}.csv'
        
        if not self.audio_tones_csv.exists():
            with open(self.audio_tones_csv, 'w', newline='') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'timestamp_utc', 'minute_boundary',
                    'power_400_hz_db', 'power_500_hz_db', 'power_600_hz_db', 'power_700_hz_db',
                    'power_1000_hz_db', 'power_1200_hz_db',
                    'ratio_500_600_db', 'ratio_400_700_db',
                    'wwv_intermod_db', 'wwvh_intermod_db',
                    'intermod_dominant', 'intermod_confidence'
                ])
            logger.info(f"Created audio tones CSV: {self.audio_tones_csv}")
    
    def _write_audio_tones(self, minute_boundary: int, iq_samples: np.ndarray):
        """Analyze and write audio tone powers with intermodulation."""
        try:
            from .audio_tone_monitor import AudioToneMonitor
            
            today = datetime.now(timezone.utc).strftime('%Y%m%d')
            file_channel = self._get_file_channel_name()
            expected_csv = self.audio_tones_dir / f'{file_channel}_audio_tones_{today}.csv'
            if self.audio_tones_csv != expected_csv:
                self.audio_tones_csv = expected_csv
                self._init_audio_tones_csv()
            
            # Analyze audio tones
            monitor = AudioToneMonitor(self.channel_name, self.sample_rate)
            analysis = monitor.analyze_minute(iq_samples, minute_boundary)
            
            with open(self.audio_tones_csv, 'a', newline='') as f:
                writer = csv.writer(f)
                utc_time = datetime.fromtimestamp(minute_boundary, timezone.utc).isoformat()
                writer.writerow([
                    utc_time,
                    minute_boundary,
                    round(analysis.power_400_hz_db, 2),
                    round(analysis.power_500_hz_db, 2),
                    round(analysis.power_600_hz_db, 2),
                    round(analysis.power_700_hz_db, 2),
                    round(analysis.power_1000_hz_db, 2),
                    round(analysis.power_1200_hz_db, 2),
                    round(analysis.ratio_500_600_db, 2),
                    round(analysis.ratio_400_700_db, 2),
                    round(analysis.wwv_intermod_500_to_600_db, 2),
                    round(analysis.wwvh_intermod_600_to_500_db, 2),
                    analysis.intermod_dominant_station or '',
                    round(analysis.intermod_confidence, 3) if analysis.intermod_confidence else ''
                ])
        except Exception as e:
            logger.error(f"Failed to write audio tones: {e}")
    
    def _read_drf_minute(self, target_minute: int):
        """
        Read one minute of data from the binary archive (or DRF fallback).
        
        First tries binary format (raw_buffer), then falls back to DRF.
        
        Args:
            target_minute: Unix timestamp of minute boundary
            
        Returns:
            Tuple of (iq_samples, system_time, rtp_timestamp) or None if not available
        """
        # Try binary format first (new format)
        result = self._read_binary_minute(target_minute)
        if result is not None:
            return result
        
        # Fall back to DRF (legacy format)
        return self._read_drf_minute_legacy(target_minute)
    
    def _read_binary_minute(self, target_minute: int):
        """Read from binary archive format."""
        from datetime import datetime, timezone
        
        # Binary files are in raw_buffer directory
        # Path: {data_root}/raw_buffer/{channel}/YYYYMMDD/{minute}.bin
        binary_dir = self.archive_dir.parent.parent / 'raw_buffer'
        channel_dir = binary_dir / self.channel_name.replace(' ', '_').replace('.', '_')
        
        dt = datetime.fromtimestamp(target_minute, tz=timezone.utc)
        date_str = dt.strftime('%Y%m%d')
        
        bin_path = channel_dir / date_str / f"{target_minute}.bin"
        json_path = channel_dir / date_str / f"{target_minute}.json"
        
        if not bin_path.exists():
            logger.debug(f"Binary file not found: {bin_path}")
            return None
        
        try:
            # Read metadata
            if json_path.exists():
                with open(json_path) as f:
                    metadata = json.load(f)
                samples_written = metadata.get('samples_written', 0)
            else:
                samples_written = bin_path.stat().st_size // 8  # complex64 = 8 bytes
            
            # Memory-map the binary file for zero-copy reading
            iq_samples = np.memmap(bin_path, dtype=np.complex64, mode='r')
            
            samples_per_minute = self.sample_rate * 60
            if len(iq_samples) < samples_per_minute * 0.9:  # Need at least 90%
                logger.debug(f"Incomplete minute: {len(iq_samples)}/{samples_per_minute}")
                return None
            
            # Pad if slightly short
            if len(iq_samples) < samples_per_minute:
                padded = np.zeros(samples_per_minute, dtype=np.complex64)
                padded[:len(iq_samples)] = iq_samples
                iq_samples = padded
            
            system_time = float(target_minute)
            # Use actual RTP timestamp from metadata, not synthesized from Unix time
            if json_path.exists() and 'start_rtp_timestamp' in metadata:
                rtp_timestamp = int(metadata['start_rtp_timestamp'])
            else:
                # Fallback: synthesize from Unix time (less accurate)
                rtp_timestamp = int(target_minute * self.sample_rate)
                logger.warning(f"No RTP timestamp in metadata, using synthesized value")
            
            logger.debug(f"Read {len(iq_samples)} samples from binary for minute {target_minute}")
            return iq_samples, system_time, rtp_timestamp
            
        except Exception as e:
            logger.debug(f"Error reading binary: {e}")
            return None
    
    def _read_drf_minute_legacy(self, target_minute: int):
        """Read from legacy DRF format (fallback)."""
        try:
            import digital_rf as drf
        except ImportError:
            return None
        
        if not self.archive_dir.exists():
            return None
        
        try:
            reader = drf.DigitalRFReader(str(self.archive_dir))
            channels = reader.get_channels()
            
            if not channels:
                return None
            
            target_start_index = int(target_minute * self.sample_rate)
            samples_per_minute = self.sample_rate * 60
            
            channel = None
            bounds = None
            for ch in sorted(channels, reverse=True):
                ch_bounds = reader.get_bounds(ch)
                if ch_bounds[0] is not None and ch_bounds[1] is not None:
                    if ch_bounds[0] <= target_start_index < ch_bounds[1]:
                        channel = ch
                        bounds = ch_bounds
                        break
                    if bounds is None or ch_bounds[1] > bounds[1]:
                        channel = ch
                        bounds = ch_bounds
            
            if channel is None or bounds is None:
                return None
            
            if target_start_index < bounds[0] or target_start_index >= bounds[1]:
                return None
            
            iq_samples = reader.read_vector(target_start_index, samples_per_minute, channel)
            
            if iq_samples is None or len(iq_samples) < samples_per_minute:
                return None
            
            iq_samples = iq_samples.squeeze().astype(np.complex64)
            system_time = target_start_index / self.sample_rate
            rtp_timestamp = target_start_index
            
            return iq_samples, system_time, rtp_timestamp
            
        except Exception as e:
            logger.debug(f"Error reading DRF: {e}")
            return None
    
    def _get_latest_minute(self) -> int:
        """Get the latest complete minute boundary from available data."""
        # Try binary format first
        latest = self._get_latest_binary_minute()
        if latest is not None:
            return latest
        
        # Fall back to DRF
        try:
            import digital_rf as drf
            reader = drf.DigitalRFReader(str(self.archive_dir))
            channels = reader.get_channels()
            
            if channels:
                latest_sample = None
                for ch in channels:
                    bounds = reader.get_bounds(ch)
                    if bounds[1] is not None:
                        if latest_sample is None or bounds[1] > latest_sample:
                            latest_sample = bounds[1]
                
                if latest_sample is not None:
                    latest_time = latest_sample / self.sample_rate
                    latest_minute = ((int(latest_time) // 60) - 3) * 60
                    return latest_minute
        except Exception as e:
            logger.debug(f"Could not get DRF bounds: {e}")
        
        # Fallback to wall-clock time
        now = time.time()
        return ((int(now) // 60) - 2) * 60
    
    def _get_latest_binary_minute(self) -> Optional[int]:
        """Get latest minute from binary archive."""
        from datetime import datetime, timezone
        
        binary_dir = self.archive_dir.parent.parent / 'raw_buffer'
        channel_dir = binary_dir / self.channel_name.replace(' ', '_').replace('.', '_')
        
        if not channel_dir.exists():
            return None
        
        # Check today's directory
        today = datetime.now(timezone.utc).strftime('%Y%m%d')
        day_dir = channel_dir / today
        
        if not day_dir.exists():
            return None
        
        # Find latest .bin file
        bin_files = list(day_dir.glob('*.bin'))
        if not bin_files:
            return None
        
        # Extract minute boundaries from filenames
        minutes = []
        for f in bin_files:
            try:
                minutes.append(int(f.stem))
            except ValueError:
                pass
        
        if not minutes:
            return None
        
        # Return second-to-last (last might be incomplete)
        # Go back 2 minutes for safety margin
        latest = max(minutes)
        return latest - 120  # 2 minutes behind
    
    def _calculate_carrier_snr(self, iq_samples: np.ndarray) -> float:
        """
        Calculate carrier SNR from IQ samples.
        
        This measures the signal-to-noise ratio of the carrier (DC component)
        which is independent of tone detection. Works for all channels.
        
        Method: Compare mean power (signal) to variance (noise fluctuations)
        
        Args:
            iq_samples: Complex IQ samples
            
        Returns:
            SNR in dB
        """
        # Calculate carrier power (mean of |IQ|^2)
        power = np.abs(iq_samples) ** 2
        carrier_power = np.mean(power)
        
        # Estimate noise power from variance of power (fluctuations around mean)
        noise_power = np.var(power)
        
        # Avoid division by zero
        if noise_power < 1e-20:
            noise_power = 1e-20
        
        # SNR in dB
        snr_db = 10 * np.log10(carrier_power / noise_power)
        
        return float(snr_db)
    
    def _decimate_to_10hz(self, iq_samples: np.ndarray, minute_boundary: int,
                           d_clock_ms: float = 0.0, uncertainty_ms: float = 999.0,
                           quality_grade: str = 'X', gap_samples: int = 0) -> bool:
        """
        Decimate 20kHz IQ to 10Hz and store in binary buffer.
        
        Args:
            iq_samples: 20kHz complex IQ samples (1 minute = 1.2M samples)
            minute_boundary: Unix timestamp of minute start
            d_clock_ms: D_clock correction from Phase 2
            uncertainty_ms: Timing uncertainty
            quality_grade: Quality grade (A-X)
            gap_samples: Number of gap samples detected
            
        Returns:
            True if decimation succeeded
        """
        if self.decimator is None:
            logger.debug("Decimation skipped - no decimator configured")
            return False
        
        try:
            # Apply high-quality decimation filter: 20kHz â†’ 10Hz (stateful)
            decimated_iq = self.decimator.process(iq_samples)
            
            if decimated_iq is None or len(decimated_iq) == 0:
                logger.warning(f"Decimation produced no output for minute {minute_boundary}")
                return False
            
            # Ensure exactly 600 samples (10 Hz Ã— 60 seconds)
            expected_samples = 600
            if len(decimated_iq) != expected_samples:
                logger.debug(f"Decimation produced {len(decimated_iq)} samples, expected {expected_samples}")
                # Pad or truncate
                if len(decimated_iq) < expected_samples:
                    padded = np.zeros(expected_samples, dtype=np.complex64)
                    padded[:len(decimated_iq)] = decimated_iq
                    decimated_iq = padded
                else:
                    decimated_iq = decimated_iq[:expected_samples]
            
            # Write to binary buffer with metadata
            success = self.decimated_buffer.write_minute(
                minute_utc=float(minute_boundary),
                decimated_iq=decimated_iq.astype(np.complex64),
                d_clock_ms=d_clock_ms,
                uncertainty_ms=uncertainty_ms,
                quality_grade=quality_grade,
                gap_samples=gap_samples
            )
            
            if success:
                logger.debug(f"Decimated minute {minute_boundary}: {len(decimated_iq)} samples")
            
            return success
            
        except Exception as e:
            logger.error(f"Decimation error at {minute_boundary}: {e}")
            return False
    
    def _write_status(self):
        """Write status file for web-ui monitoring."""
        try:
            # Build time_snap info from last result
            time_snap_dict = None
            if self.last_result and self.last_result.time_snap:
                ts = self.last_result.time_snap
                time_snap_dict = {
                    'established': True,
                    'utc_timestamp': time.time(),
                    'source': ts.anchor_station or 'unknown',
                    'confidence': ts.anchor_confidence
                }
            
            status = {
                'service': 'phase2_analytics_service',
                'version': '2.0',
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'uptime_seconds': int(time.time() - self.start_time),
                'pid': None,
                'channels': {
                    self.channel_name: {
                        'channel_name': self.channel_name,
                        'frequency_hz': self.frequency_hz,
                        'minutes_processed': self.minutes_processed,
                        'last_processed_time': datetime.fromtimestamp(
                            self.last_processed_minute, timezone.utc
                        ).isoformat() if self.last_processed_minute else None,
                        'time_snap': time_snap_dict,
                        'quality_metrics': {
                            'last_completeness_pct': 100.0 if self.last_result else 0.0,
                            'last_packet_loss_pct': 0.0,
                            'last_snr_db': self.last_carrier_snr_db  # Carrier SNR from IQ data
                        }
                    }
                },
                'overall': {
                    'channels_processing': 1,
                    'total_minutes_processed': self.minutes_processed
                }
            }
            
            # Add D_clock result if available
            if self.last_result:
                status['channels'][self.channel_name]['d_clock_ms'] = self.last_result.d_clock_ms
                # Issue 6.2: quality_grade replaced with uncertainty_ms
                # Compute backwards-compatible grade from uncertainty for web UI
                unc = self.last_result.uncertainty_ms
                if unc < 1.0:
                    quality_grade = 'A'
                elif unc < 3.0:
                    quality_grade = 'B'
                elif unc < 10.0:
                    quality_grade = 'C'
                else:
                    quality_grade = 'D'
                status['channels'][self.channel_name]['quality_grade'] = quality_grade
                status['channels'][self.channel_name]['uncertainty_ms'] = unc
                status['channels'][self.channel_name]['confidence'] = self.last_result.confidence
                if self.last_result.solution:
                    sol = self.last_result.solution
                    status['channels'][self.channel_name]['station'] = sol.station
                    status['channels'][self.channel_name]['propagation_mode'] = sol.propagation_mode
                    status['channels'][self.channel_name]['propagation_delay_ms'] = getattr(sol, 't_propagation_ms', 0)
                    status['channels'][self.channel_name]['n_hops'] = sol.n_hops
                    # Mode candidates for Mode Ridge visualization
                    status['channels'][self.channel_name]['mode_candidates'] = getattr(sol, 'mode_candidates', [])
            
            # Add convergence model state - shows convergence progress and lock status
            if hasattr(self, 'last_convergence_result') and self.last_convergence_result:
                conv = self.last_convergence_result
                status['channels'][self.channel_name]['convergence'] = {
                    'state': conv.state.value,
                    'is_locked': conv.is_locked,
                    'sample_count': conv.sample_count,
                    'uncertainty_ms': conv.uncertainty_ms if conv.uncertainty_ms != float('inf') else None,
                    'convergence_progress': conv.convergence_progress,
                    'residual_ms': conv.residual_ms,
                    'is_anomaly': conv.is_anomaly
                }
                # Also expose uncertainty at channel level for consensus weighting
                status['channels'][self.channel_name]['uncertainty_ms'] = (
                    conv.uncertainty_ms if conv.uncertainty_ms != float('inf') else 100.0
                )
            
            # Write atomically
            temp_file = self.status_file.with_suffix('.tmp')
            with open(temp_file, 'w') as f:
                json.dump(status, f, indent=2)
            temp_file.replace(self.status_file)
            
        except Exception as e:
            logger.error(f"Failed to write status: {e}")
    
    def process_minute(self, minute_boundary: int) -> bool:
        """
        Process one minute of data.
        
        Args:
            minute_boundary: Unix timestamp of minute start
            
        Returns:
            True if processed successfully
        """
        if minute_boundary in self.processed_minutes:
            return False
        
        # Read DRF data for this minute
        data = self._read_drf_minute(minute_boundary)
        if data is None:
            logger.debug(f"No data available for minute {minute_boundary}")
            return False
        
        iq_samples, system_time, rtp_timestamp = data
        
        # Always calculate carrier SNR and power from the raw IQ samples
        # This works for all channels regardless of tone detection
        self.last_carrier_snr_db = self._calculate_carrier_snr(iq_samples)
        
        # Calculate carrier power in dB (for power graphs)
        power_linear = np.mean(np.abs(iq_samples) ** 2)
        self.last_carrier_power_db = 10 * np.log10(power_linear + 1e-12)
        
        # Detect gaps in source data (zeros indicate gaps from Phase 1)
        zero_mask = (iq_samples.real == 0) & (iq_samples.imag == 0)
        gap_samples = int(np.sum(zero_mask))
        
        try:
            # Process through Phase 2 engine
            result = self.engine.process_minute(
                iq_samples=iq_samples,
                system_time=system_time,
                rtp_timestamp=rtp_timestamp
            )
            
            self.minutes_processed += 1
            self.last_processed_minute = minute_boundary
            self.processed_minutes.add(minute_boundary)
            
            if result:
                self.last_result = result
                
                # Write to CSV time series (coordinated path)
                self._write_clock_offset(result, minute_boundary, rtp_timestamp)
                
                # Write carrier power for power graphs
                solution = result.solution if hasattr(result, 'solution') else None
                time_snap = result.time_snap if hasattr(result, 'time_snap') else None
                channel_char = result.channel if hasattr(result, 'channel') else None
                
                # Compute quality_grade from uncertainty for logging/CSV
                result_unc = result.uncertainty_ms
                result_grade = 'A' if result_unc < 1.0 else 'B' if result_unc < 3.0 else 'C' if result_unc < 10.0 else 'D'
                
                self._write_carrier_power(
                    minute_boundary=minute_boundary,
                    power_db=self.last_carrier_power_db,
                    snr_db=self.last_carrier_snr_db,
                    wwv_tone_db=time_snap.wwv_snr_db if time_snap else None,
                    wwvh_tone_db=time_snap.wwvh_snr_db if time_snap else None,
                    station=solution.station if solution else None,
                    quality_grade=result_grade
                )
                
                # Write discrimination method CSVs
                if time_snap:
                    self._write_tone_detections(minute_boundary, time_snap)
                
                if channel_char:
                    self._write_bcd_discrimination(minute_boundary, channel_char)
                    self._write_doppler(minute_boundary, channel_char)
                    self._write_station_id(minute_boundary, channel_char)
                    self._write_discrimination(minute_boundary, result, time_snap, channel_char)
                
                logger.info(
                    f"Processed minute {minute_boundary}: "
                    f"D_clock={result.d_clock_ms:+.2f}ms, "
                    f"uncertainty={result_unc:.1f}ms, "
                    f"carrier_snr={self.last_carrier_snr_db:.1f}dB"
                )
            else:
                logger.debug(
                    f"Processed minute {minute_boundary}: no timing result, "
                    f"carrier_snr={self.last_carrier_snr_db:.1f}dB"
                )
            
            # Write test signal for minutes 8 and 44 (channel sounding minutes)
            # Run OUTSIDE of if result: block since test signal detection doesn't need timing lock
            minute_number = (minute_boundary // 60) % 60
            if minute_number in [8, 44]:
                self._write_test_signal(minute_boundary, iq_samples, minute_number)
            
            # Write audio tones (500/600 Hz + intermodulation) for every minute
            self._write_audio_tones(minute_boundary, iq_samples)
            
            # Decimate to 10 Hz and store in binary buffer (for spectrograms and daily upload)
            # Pass Phase 2 results for metadata
            if result:
                self._decimate_to_10hz(
                    iq_samples=iq_samples,
                    minute_boundary=minute_boundary,
                    d_clock_ms=result.d_clock_ms,
                    uncertainty_ms=getattr(self, 'last_convergence_result', None) and 
                                   self.last_convergence_result.uncertainty_ms or 999.0,
                    quality_grade=result_grade,  # Use computed grade from uncertainty
                    gap_samples=gap_samples
                )
            else:
                # No Phase 2 result - still store decimated data for completeness
                self._decimate_to_10hz(
                    iq_samples=iq_samples,
                    minute_boundary=minute_boundary,
                    d_clock_ms=0.0,
                    uncertainty_ms=999.0,
                    quality_grade='X',
                    gap_samples=gap_samples
                )
            
            return True
                
        except Exception as e:
            logger.error(f"Error processing minute {minute_boundary}: {e}")
            return False
    
    def run(self):
        """Main service loop."""
        self.running = True
        logger.info(f"Starting Phase 2 analytics service for {self.channel_name}")
        
        while self.running:
            try:
                # Get latest complete minute
                latest_minute = self._get_latest_minute()
                
                # Process any unprocessed minutes
                if latest_minute not in self.processed_minutes:
                    self.process_minute(latest_minute)
                
                # Write status
                self._write_status()
                
                # Sleep until next poll
                time.sleep(self.poll_interval)
                
            except Exception as e:
                logger.error(f"Error in main loop: {e}")
                time.sleep(self.poll_interval)
        
        logger.info("Phase 2 analytics service stopped")
    
    def stop(self):
        """Stop the service."""
        self.running = False


def main():
    """Command-line entry point."""
    parser = argparse.ArgumentParser(
        description='Phase 2 Analytics Service - Process Digital RF to timing products'
    )
    parser.add_argument('--archive-dir', required=True, help='Digital RF archive directory')
    parser.add_argument('--output-dir', required=True, help='Output directory')
    parser.add_argument('--channel-name', required=True, help='Channel name')
    parser.add_argument('--frequency-hz', type=float, required=True, help='Center frequency')
    parser.add_argument('--sample-rate', type=int, default=20000, help='Sample rate')
    parser.add_argument('--grid-square', default='', help='Receiver grid square')
    parser.add_argument('--poll-interval', type=float, default=10.0, help='Poll interval')
    parser.add_argument('--log-level', default='INFO', help='Log level')
    
    # Additional args for compatibility with grape-analytics.sh
    parser.add_argument('--state-file', help='State file (not used)')
    parser.add_argument('--backfill-gaps', action='store_true', help='Backfill gaps (not used)')
    parser.add_argument('--max-backfill', type=int, help='Max backfill (not used)')
    parser.add_argument('--callsign', help='Callsign')
    parser.add_argument('--receiver-name', help='Receiver name')
    parser.add_argument('--psws-station-id', help='PSWS station ID')
    parser.add_argument('--psws-instrument-id', help='PSWS instrument ID')
    parser.add_argument('--latitude', type=float, help='Precise latitude (improves timing ~16Î¼s)')
    parser.add_argument('--longitude', type=float, help='Precise longitude (improves timing ~16Î¼s)')
    
    args = parser.parse_args()
    
    # Setup logging
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format='%(asctime)s %(levelname)s:%(name)s:%(message)s'
    )
    
    # Build station config
    station_config = {
        'callsign': args.callsign,
        'grid_square': args.grid_square,
        'receiver_name': args.receiver_name,
        'station_id': args.psws_station_id,
        'instrument_id': args.psws_instrument_id,
        'latitude': args.latitude,
        'longitude': args.longitude
    }
    
    # Create service
    service = Phase2AnalyticsService(
        archive_dir=Path(args.archive_dir),
        output_dir=Path(args.output_dir),
        channel_name=args.channel_name,
        frequency_hz=args.frequency_hz,
        sample_rate=args.sample_rate,
        receiver_grid=args.grid_square,
        station_config=station_config,
        poll_interval=args.poll_interval
    )
    
    # Handle signals
    def signal_handler(signum, frame):
        logger.info(f"Received signal {signum}, stopping...")
        service.stop()
    
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    
    # Run
    service.run()


if __name__ == '__main__':
    main()
