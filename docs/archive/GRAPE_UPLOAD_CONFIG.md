# GRAPE Upload Configuration & Testing

## âœ… All 6 Fixes Implemented

The uploader now matches wsprdaemon's behavior for PSWS uploads:

1. âœ… **SFTP Protocol** - Bandwidth-limited SFTP with trigger directories
2. âœ… **24-Hour Wait** - Only uploads completed days (yesterday or earlier)
3. âœ… **Completion Markers** - `.upload_complete` prevents re-uploading
4. âœ… **Digital RF Validation** - Verifies data integrity before upload
5. âœ… **Trigger Directories** - `cOBS_#INST_#TIMESTAMP` format
6. âœ… **Smart Enqueue** - Comprehensive validation with detailed logging

---

## Configuration

### Your PSWS Credentials

Based on your setup:
- **Station ID**: `S000171`
- **Instrument ID**: `172`
- **SSH Key**: Already set up with `ssh-copy-id`

### Update `config.yaml`

Add or update the upload section:

```yaml
upload:
  enabled: true
  protocol: "sftp"                    # Use SFTP (default)
  host: "pswsnetwork.eng.ua.edu"    # PSWS server
  user: "S000171"                     # Your station ID
  
  ssh:
    key_file: "/home/mjh/.ssh/id_rsa"  # Path to your SSH key
  
  bandwidth_limit_kbps: 100          # 100 KB/s (adjust as needed)
  max_retries: 5
  retry_backoff_base: 2              # Exponential backoff (2^n minutes)
  
  queue_file: "/var/lib/signal-recorder/upload_queue.json"

# Metadata for PSWS
station:
  callsign: "YOUR_CALL"              # Your callsign
  grid_square: "YOUR_GRID"           # Your Maidenhead grid (e.g., EM00)
  station_id: "S000171"              # PSWS station ID
  instrument_id: "172"               # PSWS instrument ID
```

---

## Testing with Synthetic Data

### Step 1: Create Test Data

```bash
cd ~/git/signal-recorder
python3 test_upload.py --verify

# Output:
# Creating test Digital RF dataset:
#   Date: 2025-10-26
#   Channel: WWV_2_5
#   Duration: 1 minute(s)
#   Samples: 600
#   ...
# âœ… Test dataset created successfully!
# Dataset: /tmp/grape_test_upload/OBS2025-10-26T00-00
```

### Step 2: Verify SFTP Connection

```bash
# Test manual SFTP (should work since you did ssh-copy-id)
sftp S000171@pswsnetwork.eng.ua.edu

# If successful, you'll see:
# sftp>

# Try listing:
# sftp> ls

# Quit:
# sftp> quit
```

### Step 3: Test Upload (Manual)

Create a test script to exercise the uploader:

```bash
cd ~/git/signal-recorder
cat > test_manual_upload.py << 'EOF'
#!/usr/bin/env python3
"""Test manual upload with synthetic data"""

import sys
from pathlib import Path
from datetime import datetime, timezone, timedelta
from signal_recorder.uploader import UploadManager

# Configuration (matching your setup)
config = {
    'protocol': 'sftp',
    'host': 'pswsnetwork.eng.ua.edu',
    'user': 'S000171',
    'ssh': {'key_file': str(Path.home() / '.ssh/id_rsa')},
    'bandwidth_limit_kbps': 100,
    'max_retries': 3,
    'queue_file': '/tmp/test_upload_queue.json'
}

# Metadata
metadata = {
    'date': (datetime.now(timezone.utc) - timedelta(days=1)).date(),
    'callsign': 'TEST',
    'grid_square': 'EM00',
    'station_id': 'S000171',
    'instrument_id': '172'
}

# Test dataset path
dataset_path = Path('/tmp/grape_test_upload/OBS2025-10-26T00-00')

if not dataset_path.exists():
    print(f"Error: Test dataset not found at {dataset_path}")
    print("Run: python3 test_upload.py --verify")
    sys.exit(1)

print(f"Testing upload of: {dataset_path}")
print(f"Metadata: {metadata}")

# Create uploader (no storage manager needed for testing)
class DummyStorage:
    pass

uploader = UploadManager(config, DummyStorage())

# Enqueue dataset
print("\n1. Enqueuing dataset...")
uploader.enqueue(dataset_path, metadata)

# Process queue
print("\n2. Processing upload queue...")
uploader.process_queue()

# Check status
print("\n3. Upload queue status:")
status = uploader.get_status()
for key, value in status.items():
    print(f"   {key}: {value}")

print("\nâœ… Test complete!")
print("\nCheck PSWS dashboard to verify upload was processed.")
EOF

chmod +x test_manual_upload.py
```

### Step 4: Run Upload Test

```bash
python3 test_manual_upload.py
```

**Expected Output:**
```
Testing upload of: /tmp/grape_test_upload/OBS2025-10-26T00-00
Metadata: {'date': datetime.date(2025, 10, 26), ...}

1. Enqueuing dataset...
INFO: Validating 1 channels in /tmp/grape_test_upload/OBS2025-10-26T00-00
INFO: Channel WWV_2_5: 600 samples valid
INFO: âœ… Digital RF validation passed
INFO: âœ… Enqueued upload: /tmp/grape_test_upload/OBS2025-10-26T00-00
   Date: 2025-10-26
   Remote: OBS2025-10-26T00-00

2. Processing upload queue...
INFO: Upload attempt 1/3: /tmp/grape_test_upload/OBS2025-10-26T00-00
INFO: Uploading /tmp/grape_test_upload/OBS2025-10-26T00-00 via SFTP to S000171@pswsnetwork.eng.ua.edu
INFO: Trigger directory: cOBS2025-10-26T00-00_#172_#2025-10-27T16-30
INFO: SFTP upload successful
INFO: âœ… Upload verified
INFO: Created upload completion marker: /tmp/grape_test_upload/.upload_complete

3. Upload queue status:
   total: 1
   pending: 0
   uploading: 0
   completed: 1
   failed: 0

âœ… Test complete!
```

### Step 5: Verify on PSWS

1. Check your email (PSWS sends notifications)
2. Visit PSWS dashboard
3. Look for your station (S000171)
4. Verify dataset appears with timestamp

---

## Integration with signal-recorder

Once testing succeeds, integrate into the main recorder:

### Option A: Automatic Upload (After Midnight)

The recorder will automatically enqueue datasets for upload after midnight UTC rolls over.

### Option B: Manual Trigger

Add an endpoint to manually trigger upload:

```python
# In app.py
@app.post("/api/upload/trigger")
def trigger_upload():
    """Manually trigger upload of completed datasets"""
    if not hasattr(app.state, 'uploader'):
        return {"error": "Upload manager not configured"}
    
    # Scan for completed datasets
    output_dir = Path(config['output']['base_dir'])
    datasets = [d for d in output_dir.glob('OBS*') if d.is_dir()]
    
    for dataset in datasets:
        # Extract date from OBS directory name
        # OBS2025-10-26T00-00 â†’ 2025-10-26
        date_str = dataset.name[3:13]  # YYYY-MM-DD
        
        metadata = {
            'date': date_str,
            'callsign': config['station']['callsign'],
            'grid_square': config['station']['grid_square'],
            'station_id': config['station']['station_id'],
            'instrument_id': config['station']['instrument_id']
        }
        
        app.state.uploader.enqueue(dataset, metadata)
    
    app.state.uploader.process_queue()
    
    status = app.state.uploader.get_status()
    return {"status": "triggered", "queue": status}
```

---

## Monitoring Upload Status

### Check Queue

```bash
# View upload queue
cat /var/lib/signal-recorder/upload_queue.json

# Or via API (if implemented)
curl http://localhost:8000/api/upload/status
```

### Check Logs

```bash
# Upload logs
tail -f /tmp/signal-recorder-daemon.log | grep -i upload

# Look for:
# - "âœ… Enqueued upload"
# - "SFTP upload successful"
# - "Upload verified"
# - "Created upload completion marker"
```

### Check Completion Markers

```bash
# List completed uploads
find /path/to/output -name ".upload_complete"

# Each directory with this file has been successfully uploaded
```

---

## Troubleshooting

### Issue: "Skipping: date is not before today"

**Cause**: Trying to upload today's data  
**Fix**: Wait until midnight UTC, or use test data from yesterday

### Issue: "Already uploaded (.upload_complete marker exists)"

**Cause**: Dataset already uploaded  
**Fix**: Normal behavior - prevents re-uploading. To force re-upload, delete `.upload_complete` file

### Issue: "Digital RF validation failed"

**Cause**: Incomplete or corrupt Digital RF files  
**Fix**: Check daemon logs for Digital RF writing errors

### Issue: "SFTP failed: Permission denied"

**Cause**: SSH key not working  
**Fix**:
```bash
# Re-test SSH connection
ssh S000171@pswsnetwork.eng.ua.edu

# If password prompt, re-run ssh-copy-id
ssh-copy-id S000171@pswsnetwork.eng.ua.edu
```

### Issue: "SFTP timeout after 1 hour"

**Cause**: Very slow upload or network issues  
**Fix**: Check bandwidth_limit_kbps, increase timeout in code if needed

---

## Expected Behavior

### Daily Upload Cycle

1. **00:00 UTC**: Day rolls over, new date starts recording
2. **00:00 UTC + buffer**: Previous day's Digital RF finalized
3. **Enqueue**: Dataset from yesterday is validated and enqueued
4. **Upload**: SFTP transfer begins (bandwidth-limited)
5. **Trigger**: PSWS receives trigger directory
6. **Processing**: PSWS processes and archives data
7. **Marker**: `.upload_complete` created locally

### Bandwidth Usage

With 100 KB/s limit:
- 1 minute of data (~7 KB) uploads in < 1 second
- Full 24-hour dataset (~10 MB) uploads in ~100 seconds
- Multiple channels upload sequentially

### Retry Behavior

- **Attempt 1**: Immediate
- **Attempt 2**: 2 minutes later (2^1)
- **Attempt 3**: 4 minutes later (2^2)
- **Attempt 4**: 8 minutes later (2^3)
- **Attempt 5**: 16 minutes later (2^4)
- **Failed**: Marked as failed after 5 attempts

---

## Next Steps

1. âœ… Pull latest code on bee1
2. âœ… Update config.yaml with your credentials  
3. âœ… Run test_upload.py to create test data
4. âœ… Test SFTP connection manually
5. âœ… Run test_manual_upload.py
6. âœ… Verify on PSWS dashboard
7. âœ… Integrate into main recorder
8. âœ… Monitor first real 24-hour upload

---

## Success Checklist

- [ ] SFTP connection tested manually
- [ ] Test data uploaded successfully  
- [ ] Trigger directory created on PSWS
- [ ] PSWS dashboard shows uploaded data
- [ ] `.upload_complete` marker file created
- [ ] Re-upload attempt correctly skipped
- [ ] Logs show validation steps
- [ ] Bandwidth limiting works as expected

**All systems ready for production GRAPE uploads!** ðŸš€
